#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Jun  6 14:17:07 2023@author: mcgoug01"""from torchvision import modelsimport torch.nn as nnimport torchimport copydef return_efficientnet(size='small',dev='cpu',in_channels=1,out_channels=3):    if size == 'small':        model_generator = models.efficientnet_v2_s        weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1    elif size == 'medium':        model_generator = models.efficientnet_v2_m        weights = models.EfficientNet_V2_M_Weights.IMAGENET1K_V1    elif size == 'large':        model_generator = models.efficientnet_v2_l        weights = models.EfficientNet_V2_L_Weights.IMAGENET1K_V1    else:        assert(1==2)    axial_tile_model = model_generator(weights = weights).to(dev)        if size == 'large':        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False).to(dev)    else:        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False).to(dev)            axial_tile_model.classifier[-1]= nn.Linear(1280,out_channels,bias=True).to(dev)         return axial_tile_model.to(dev)def return_swin(size='small',dev='cpu',in_channels=1,out_channels=3):    if size == 'small':        model_generator = models.swin_v2_t        weights = models.Swin_V2_T_Weights.IMAGENET1K_V1    elif size == 'medium':        model_generator = models.swin_v2_s        weights = models.Swin_V2_S_Weights.IMAGENET1K_V1    elif size == 'large':        model_generator = models.swin_v2_b        weights = models.Swin_V2_B_Weights.IMAGENET1K_V1    else:        assert(1==2)    axial_tile_model = model_generator(weights = weights).to(dev)    if size == 'large':        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,128,kernel_size=(4, 4), stride=(4, 4)).to(dev)        axial_tile_model.head= nn.Linear(1024,out_channels,bias=True).to(dev)     else:        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,96,kernel_size=(4, 4), stride=(4, 4)).to(dev)        axial_tile_model.head= nn.Linear(768,out_channels,bias=True).to(dev)         return axial_tile_model.to(dev)def return_vit(size='small',dev='cpu',in_channels=1,out_channels=3):    if size == 'small':        model_generator = models.vit_b_16        weights = models.ViT_B_16_Weights.IMAGENET1K_V1    elif size == 'medium':        model_generator = models.vit_b_32        weights = models.ViT_B_32_Weights.IMAGENET1K_V1    elif size == 'large':        model_generator = models.vit_l_32        weights = models.ViT_L_32_Weights.IMAGENET1K_V1    else:        assert(1==2)            axial_tile_model = model_generator(weights = weights).to(dev)    if size == 'large':        axial_tile_model.conv_proj = nn.Conv2d(in_channels,1024,kernel_size=(32, 32), stride=(32, 32)).to(dev)        axial_tile_model.heads.head= nn.Linear(1024,out_channels,bias=True).to(dev)     elif size == 'medium':        axial_tile_model.conv_proj = nn.Conv2d(in_channels,768,kernel_size=(32, 32), stride=(32, 32)).to(dev)        axial_tile_model.heads.head= nn.Linear(768,out_channels,bias=True).to(dev)     else:        axial_tile_model.conv_proj = nn.Conv2d(in_channels,768,kernel_size=(16,16), stride=(16,16)).to(dev)        axial_tile_model.heads.head= nn.Linear(768,out_channels,bias=True).to(dev)         return axial_tile_model.to(dev)def return_resnet(size='small',dev='cpu',in_channels=1,out_channels=3):    if size == 'small':        model_generator = models.resnet50        weights = models.ResNet50_Weights.IMAGENET1K_V1    elif size == 'medium':        model_generator = models.resnet101        weights = models.ResNet101_Weights.IMAGENET1K_V1    elif size == 'large':        model_generator = models.resnet152        weights = models.ResNet152_Weights.IMAGENET1K_V1    else:        assert(1==2)    axial_tile_model = model_generator(weights = weights).to(dev)        axial_tile_model.conv1 = nn.Conv2d(in_channels,64,kernel_size=(4, 4), stride=(4, 4)).to(dev)    axial_tile_model.fc= nn.Linear(2048,out_channels,bias=True).to(dev)         return axial_tile_model.to(dev)def return_resnext(size='small',dev='cpu',in_channels=1,out_channels=3):    if size == 'small':        model_generator = models.resnext50_32x4d        weights = models.ResNeXt50_32X4D_Weights.IMAGENET1K_V1    elif size == 'medium':        model_generator = models.resnext101_64x4d        weights = models.ResNeXt101_64X4D_Weights.IMAGENET1K_V1    elif size == 'large':        model_generator = models.resnext101_32x8d        weights = models.ResNeXt101_32X8D_Weights.IMAGENET1K_V1    else:        assert(1==2)            axial_tile_model = model_generator(weights = weights).to(dev)    axial_tile_model.conv1 = nn.Conv2d(in_channels,64,kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(dev)    axial_tile_model.fc= nn.Linear(2048,out_channels,bias=True).to(dev)         return axial_tile_model.to(dev)def return_convnext(size='small',dev='cpu',in_channels=1,out_channels=3):    if size == 'small':        model_generator = models.convnext_small        weights = models.ConvNeXt_Small_Weights.IMAGENET1K_V1    elif size == 'medium':        model_generator = models.convnext_base        weights = models.ConvNeXt_Base_Weights.IMAGENET1K_V1    elif size == 'large':        model_generator = models.convnext_large        weights = models.ConvNeXt_Large_Weights.IMAGENET1K_V1    else:        assert(1==2)                    axial_tile_model = model_generator(weights = weights).to(dev)        if size == 'large':        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,192,kernel_size=(4, 4), stride=(4, 4)).to(dev)        axial_tile_model.classifier[-1]= nn.Linear(1536,out_channels,bias=True).to(dev)     elif size =='medium':        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,128, kernel_size=(4, 4), stride=(4, 4)).to(dev)        axial_tile_model.classifier[-1]= nn.Linear(1024,out_channels,bias=True).to(dev)     else:        axial_tile_model.features[0][0] = nn.Conv2d(in_channels,96, kernel_size=(4, 4), stride=(4, 4)).to(dev)        axial_tile_model.classifier[-1]= nn.Linear(768,out_channels,bias=True).to(dev)     return axial_tile_model.to(dev)model_dict = {'EffNet':return_efficientnet,              'Swin':return_swin,              'ViT':return_vit,              'ResNet':return_resnet,              'ResNeXT':return_resnext,              'ConvNeXT':return_convnext}class kidneywisemodel(nn.Module):    def __init__(self,axial_tile_model,modelname='EffNet',size='small',n2=32,n1=16,                 num_labels=2,dropout=0.8,dev='cpu'):        super(kidneywisemodel,self).__init__()                # if n3!=3:        #     if modelname =='EffNet':        #         axial_tile_model.classifier[-1]= nn.Linear(1280,n3,bias=True).to(dev)         #     elif modelname =='Swin':        #         if size == 'large':        #             axial_tile_model.head= nn.Linear(1024,n3,bias=True).to(dev)         #         else:        #             axial_tile_model.head= nn.Linear(768,n3,bias=True).to(dev)         #     elif modelname =='ViT':        #         if size == 'large':        #             axial_tile_model.heads.head= nn.Linear(1024,n3,bias=True).to(dev)         #         elif size == 'medium':        #             axial_tile_model.heads.head= nn.Linear(768,n3,bias=True).to(dev)         #         else:        #             axial_tile_model.heads.head= nn.Linear(768,n3,bias=True).to(dev)         #     elif modelname == 'ResNet':        #         axial_tile_model.fc= nn.Linear(2048,n3,bias=True).to(dev)         #     elif modelname == 'ResNeXT':        #         axial_tile_model.fc= nn.Linear(2048,n3,bias=True).to(dev)         #     elif modelname == 'ConvNeXT':        #         if size == 'large':        #             axial_tile_model.classifier[-1]= nn.Linear(1536,n3,bias=True).to(dev)         #         elif size =='medium':        #             axial_tile_model.classifier[-1]= nn.Linear(1024,n3,bias=True).to(dev)         #         else:        #             axial_tile_model.classifier[-1]= nn.Linear(768,n3,bias=True).to(dev)         #     else:        #         assert(1==2)                    self.model = axial_tile_model.to(dev)        self.modelname = modelname        self.n1=n1        self.n2=n2        self.tw_conv = nn.Conv1d(3,1,15,stride=1,padding=7).to(dev)        self.tw_process = nn.Linear(n2,n1).to(dev)        self.final = nn.Linear(n1,num_labels).to(dev)                self.dropout = nn.Dropout(dropout)        self.actv = nn.ReLU()        self.tw_pool = nn.AdaptiveMaxPool1d(n2)                self.device=dev        self.softmax = nn.Softmax(dim=1)    def forward(self,tilestack):        # there is a problem here - how do we integrate the kw tile classifiers with the shape ensemble..        with torch.no_grad(): #prevent gradient flow through twCNN - reduce VRAM burden and unnecessary trainin            tile_stacks = torch.stack([self.model(tile) for tile in tilestack])            tile_stacks = tile_stacks.squeeze(dim=1)  #extract B x N x 3 features - N is num of tiles in case                tile_stacks = self.softmax(torch.swapaxes(tile_stacks,-2,-1))        tw_conv1 = self.actv(self.dropout(self.tw_conv(tile_stacks))) # Convolve over features, output should be N x 3 again        top_k = self.tw_pool(tw_conv1)         # vals,indices = torch.topk(tw_conv1,k=self.n2,dim=-1)          tw_enc = self.actv(self.tw_process(top_k)).squeeze(dim=-2)        common_16 = self.actv(self.dropout(tw_enc))        return self.final(common_16)    def replace2d_to3d(layer,dev='cpu'):    if str(type(layer))=="<class \'torch.nn.modules.linear.Linear\'>":        return layer    elif str(type(layer))=="<class \'torch.nn.modules.batchnorm.BatchNorm2d\'>":        feat,mom,eps,aff = layer.num_features,layer.momentum,layer.eps,layer.affine        # print(feat,mom,eps)        return nn.BatchNorm3d(feat,eps,mom,affine=aff,device=dev)    elif str(type(layer))=="<class \'torch.nn.modules.conv.Conv2d\'>":        inc,outc,stride,pad,kern,is_bias = layer.in_channels,layer.out_channels,layer.stride[0],layer.padding[0],layer.kernel_size[0],layer.bias==None        return nn.Conv3d(inc,outc,stride=stride,padding=pad,kernel_size=kern,bias=is_bias,device=dev)    else:        print(layer)        assert(1==2)def return_efficientnet3D(size='small',dev='cpu',in_channels=1,out_channels=3):    eff2d = return_efficientnet(size,dev,in_channels,out_channels)        completed_layers = []    for name, _ in eff2d.named_parameters():        # print(name)        split_list = name.split('.')                if len(split_list)==4:            layname,id1,id2,_ = split_list            blockname,id3,id4=[None]*3        elif len(split_list)==7:                layname,id1,id2,blockname,id3,id4,_ = split_list        elif len(split_list)==3:            layname,id1,_ = split_list            id2,blockname,id3,id4=[None]*4        else:            assert(1==2)                    layer_metadata = (id1,id2,blockname,id3,id4)        if layer_metadata in completed_layers:continue        completed_layers.append(layer_metadata)                if layname=='features':            if blockname!=None:                if id4.isnumeric():                    layer = eff2d.features[int(id1)][int(id2)].block[int(id3)][int(id4)]                                        new_layer = replace2d_to3d(layer,dev=dev)                    eff2d.features[int(id1)][int(id2)].block[int(id3)][int(id4)] = new_layer                elif id4=='fc1':                    layer = eff2d.features[int(id1)][int(id2)].block[int(id3)].fc1                    new_layer = replace2d_to3d(layer,dev=dev)                    eff2d.features[int(id1)][int(id2)].block[int(id3)].fc1 = new_layer                elif id4=='fc2':                    layer= eff2d.features[int(id1)][int(id2)].block[int(id3)].fc2                    new_layer = replace2d_to3d(layer,dev=dev)                    eff2d.features[int(id1)][int(id2)].block[int(id3)].fc2 = new_layer                else:                    assert(1==2)            else:                layer = eff2d.features[int(id1)][int(id2)]                new_layer = replace2d_to3d(layer,dev=dev)                eff2d.features[int(id1)][int(id2)] = new_layer        elif layname=='classifier':            layer = eff2d.classifier[int(id1)]            new_layer = replace2d_to3d(layer,dev=dev)            eff2d.classifier[int(id1)] = new_layer                    completed_layers.append(str(type(new_layer)))            completed_layers = list(set(completed_layers))    eff2d.avgpool = nn.AdaptiveMaxPool3d(output_size=1)        return eff2d.to(dev)def resnext_layerreplacer(layer):    newlayer = copy.copy(layer)    for subidx in range(len(layer)):        sublayer = layer[subidx]        newlayer[subidx].conv1 = replace2d_to3d(sublayer.conv1)        newlayer[subidx].conv2 = replace2d_to3d(sublayer.conv2)        newlayer[subidx].conv3 = replace2d_to3d(sublayer.conv3)        newlayer[subidx].bn1 = replace2d_to3d(sublayer.bn1)        newlayer[subidx].bn2 = replace2d_to3d(sublayer.bn2)        newlayer[subidx].bn3 = replace2d_to3d(sublayer.bn3)        if sublayer.downsample!=None:            newlayer[subidx].downsample[0] = replace2d_to3d(sublayer.downsample[0])            newlayer[subidx].downsample[1] = replace2d_to3d(sublayer.downsample[1])            return newlayer        def return_resnext3D(size='small',dev='cpu',in_channels=1,out_channels=3):    rnxt2d = return_resnext(size,dev,in_channels,out_channels)    rnxt3d = copy.deepcopy(rnxt2d)    completed_layers = []    for name, _ in rnxt2d.named_parameters():        # print(name)        split_list = name.split('.')        if len(split_list)==2:            layname,_ = split_list            blockname,id1,id2=[None]*3        elif len(split_list)==4:                blockname,id1,layname,_ = split_list            id2=None        elif len(split_list)==5:            blockname,id1,layname,id2,_ = split_list        else:            assert(1==2)                    layer_metadata = (blockname)        if layer_metadata in completed_layers:continue        completed_layers.append(layer_metadata)                        if blockname==None:            layer1 = rnxt2d.conv1            layer2 = rnxt2d.bn1            layer3 = rnxt3d.fc            rnxt3d.conv1 = replace2d_to3d(layer1,dev=dev)            rnxt3d.bn1=replace2d_to3d(layer2,dev=dev)            rnxt3d.fc=replace2d_to3d(layer3,dev=dev)        elif blockname=='layer1':            layer = rnxt2d.layer1            rnxt3d.layer1 = resnext_layerreplacer(layer)        elif blockname=='layer2':            layer = rnxt2d.layer2            rnxt3d.layer2 = resnext_layerreplacer(layer)        elif blockname=='layer3':            layer = rnxt2d.layer3            rnxt3d.layer3 = resnext_layerreplacer(layer)        elif blockname=='layer4':            layer = rnxt2d.layer4            rnxt3d.layer4 = resnext_layerreplacer(layer)                    else:            assert(1==2)            completed_layers = list(set(completed_layers))    rnxt3d.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)    rnxt3d.avgpool = nn.AdaptiveMaxPool3d(output_size=1)        return rnxt3d.to(dev)    if __name__ == '__main__':    size = 'small'    model2d =return_efficientnet(size=size)    print(sum(p.numel() for p in model2d.parameters() if p.requires_grad))    model3d = return_efficientnet3D(size=size)    print(sum(p.numel() for p in model3d.parameters() if p.requires_grad))    # kwmodel = kidneywisemodel(model,'EffNet',size)    ex = torch.ones((1,1,67,224,224))    pred = model3d(ex)    print(pred)    # pred = kwmodel(ex)    # print(pred)